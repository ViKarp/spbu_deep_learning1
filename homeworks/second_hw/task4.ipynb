{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T18:25:39.608842Z",
     "start_time": "2024-10-13T18:25:36.446408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 225)\n",
      "(225, 225)\n",
      "(225, 225)\n",
      "(225, 225)\n",
      "151875\n",
      "[[[0.         0.         0.        ]\n",
      "  [0.01960784 0.01960784 0.01960784]\n",
      "  [0.02745098 0.02745098 0.02745098]\n",
      "  ...\n",
      "  [0.29411766 0.3372549  0.25882354]\n",
      "  [0.28235295 0.32941177 0.24313726]\n",
      "  [0.25882354 0.30588236 0.23921569]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.01960784 0.01960784 0.01960784]\n",
      "  [0.04705882 0.04705882 0.04705882]\n",
      "  ...\n",
      "  [0.3019608  0.3529412  0.2627451 ]\n",
      "  [0.3137255  0.36862746 0.2784314 ]\n",
      "  [0.45882353 0.5176471  0.43137255]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.04705882 0.04705882 0.04705882]\n",
      "  [0.0627451  0.0627451  0.0627451 ]\n",
      "  ...\n",
      "  [0.4392157  0.49411765 0.4117647 ]\n",
      "  [0.4627451  0.52156866 0.44705883]\n",
      "  [0.4        0.45490196 0.39215687]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.28235295 0.29411766 0.3254902 ]\n",
      "  [0.2784314  0.2901961  0.3254902 ]\n",
      "  [0.25882354 0.27058825 0.30588236]\n",
      "  ...\n",
      "  [0.40784314 0.41568628 0.4745098 ]\n",
      "  [0.44313726 0.4509804  0.50980395]\n",
      "  [0.46666667 0.4745098  0.5372549 ]]\n",
      "\n",
      " [[0.28627452 0.29803923 0.3372549 ]\n",
      "  [0.26666668 0.28235295 0.31764707]\n",
      "  [0.2509804  0.2627451  0.29803923]\n",
      "  ...\n",
      "  [0.46666667 0.4745098  0.53333336]\n",
      "  [0.45490196 0.46666667 0.52156866]\n",
      "  [0.5176471  0.52156866 0.58431375]]\n",
      "\n",
      " [[0.2        0.20392157 0.24705882]\n",
      "  [0.2627451  0.27450982 0.3137255 ]\n",
      "  [0.33333334 0.34509805 0.38431373]\n",
      "  ...\n",
      "  [0.45882353 0.46666667 0.5254902 ]\n",
      "  [0.4509804  0.45882353 0.5176471 ]\n",
      "  [0.47058824 0.47843137 0.5372549 ]]]\n"
     ]
    }
   ],
   "source": [
    "from task4 import *\n",
    "from task3 import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Transform Pipeline 1:\n",
      "(28, 28)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomCrop' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 42\u001B[0m\n\u001B[0;32m     39\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining with Transform Pipeline 1:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 42\u001B[0m train_loss_1, train_acc_1 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m test_loss_1, test_acc_1 \u001B[38;5;241m=\u001B[39m test_model(model_1, test_loader, criterion)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining with Transform Pipeline 2:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\homeworks\\second_hw\\task4.py:63\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m     60\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     64\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Добавляем канал\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(images)\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\homeworks\\second_hw\\task4.py:25\u001B[0m, in \u001B[0;36mFashionMNISTDatasetWithTransforms.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     23\u001B[0m image \u001B[38;5;241m=\u001B[39m ToPILImage()(image)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 25\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image, label\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\homeworks\\second_hw\\task3.py:112\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28mprint\u001B[39m(image\u001B[38;5;241m.\u001B[39msize)\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m--> 112\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28mprint\u001B[39m(image\u001B[38;5;241m.\u001B[39msize)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "File \u001B[1;32m~\\Studies\\spbu_deep_learning1\\homeworks\\second_hw\\task3.py:36\u001B[0m, in \u001B[0;36mRandomCrop.__call__\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: Image\u001B[38;5;241m.\u001B[39mImage) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image\u001B[38;5;241m.\u001B[39mImage:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp:\n\u001B[1;32m---> 36\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ToPILImage()(TF\u001B[38;5;241m.\u001B[39mcenter_crop(image, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m))\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'RandomCrop' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "transform_pipeline_1 = Compose([\n",
    "    RandomCrop(p=0.7, crop_size=(24, 24)),\n",
    "    RandomRotate(p=0.6, degrees=30),\n",
    "    RandomZoom(p=0.5, zoom_range=(0.8, 1.2)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "transform_pipeline_2 = Compose([\n",
    "    RandomCrop(p=0.9, crop_size=(26, 26)),\n",
    "    RandomRotate(p=0.9, degrees=40),\n",
    "    RandomZoom(p=0.8, zoom_range=(0.7, 1.3)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "transform_pipeline_none = Compose([ToTensor()])\n",
    "\n",
    "train_dataset_with_transforms_1 = FashionMNISTDatasetWithTransforms(train_dataset, transforms=transform_pipeline_1)\n",
    "train_dataset_with_transforms_2 = FashionMNISTDatasetWithTransforms(train_dataset, transforms=transform_pipeline_2)\n",
    "train_dataset_no_transforms = FashionMNISTDatasetWithTransforms(train_dataset, transforms=transform_pipeline_none)\n",
    "\n",
    "train_loader_1 = DataLoader(train_dataset_with_transforms_1, batch_size=64, shuffle=True)\n",
    "train_loader_2 = DataLoader(train_dataset_with_transforms_2, batch_size=64, shuffle=True)\n",
    "train_loader_no_transforms = DataLoader(train_dataset_no_transforms, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(FashionMNISTDatasetWithTransforms(test_dataset, transforms=transform_pipeline_none),\n",
    "                         batch_size=64, shuffle=False)\n",
    "\n",
    "model_1 = SimpleCNN()\n",
    "model_2 = SimpleCNN()\n",
    "model_no_transforms = SimpleCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_1 = optim.Adam(model_1.parameters(), lr=0.001)\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "optimizer_no_transforms = optim.Adam(model_no_transforms.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "print(\"Training with Transform Pipeline 1:\")\n",
    "train_loss_1, train_acc_1 = train_model(model_1, train_loader_1, criterion, optimizer_1, num_epochs)\n",
    "test_loss_1, test_acc_1 = test_model(model_1, test_loader, criterion)\n",
    "\n",
    "print(\"\\nTraining with Transform Pipeline 2:\")\n",
    "train_loss_2, train_acc_2 = train_model(model_2, train_loader_2, criterion, optimizer_2, num_epochs)\n",
    "test_loss_2, test_acc_2 = test_model(model_2, test_loader, criterion)\n",
    "\n",
    "print(\"\\nTraining with No Transforms:\")\n",
    "train_loss_no, train_acc_no = train_model(model_no_transforms, train_loader_no_transforms, criterion,\n",
    "                                          optimizer_no_transforms, num_epochs)\n",
    "test_loss_no, test_acc_no = test_model(model_no_transforms, test_loader, criterion)\n",
    "\n",
    "plot_metrics(train_loss_1, train_acc_1, test_loss_1, test_acc_1, title_suffix=\"(Transform Pipeline 1)\")\n",
    "plot_metrics(train_loss_2, train_acc_2, test_loss_2, test_acc_2, title_suffix=\"(Transform Pipeline 2)\")\n",
    "plot_metrics(train_loss_no, train_acc_no, test_loss_no, test_acc_no, title_suffix=\"(No Transforms)\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-13T18:25:40.950260Z",
     "start_time": "2024-10-13T18:25:40.242578Z"
    }
   },
   "id": "ecf4e432b8818db9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "326cdcfe73ae4daa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
