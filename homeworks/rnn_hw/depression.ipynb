{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.211178Z",
     "start_time": "2024-12-20T15:42:26.040946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ],
   "id": "647eee18a0b229b0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.303833Z",
     "start_time": "2024-12-20T15:42:27.226414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"depr/train.csv\")\n",
    "df.head()"
   ],
   "id": "fecec1d752adebdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0                   Ok guys I have a dilemma for you   \n",
       "1   1        Update from my cringe overdramatic ass post   \n",
       "2   2  Don't know if this is allowed, I work in youth...   \n",
       "3   3  So I think I just ascended to a newer level of...   \n",
       "4   4                       My friend got a gf before me   \n",
       "\n",
       "                                                body  label  \n",
       "0  Ok so I‚Äôm thinking about asking out this girl ...    0.0  \n",
       "1  56 days ago from today I posted about my then-...    0.0  \n",
       "2                                                NaN    0.0  \n",
       "3  So I was rebattling the elite four in Pokemon ...    0.0  \n",
       "4    But I have more reddit karma so I think I win üòé    0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok guys I have a dilemma for you</td>\n",
       "      <td>Ok so I‚Äôm thinking about asking out this girl ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Update from my cringe overdramatic ass post</td>\n",
       "      <td>56 days ago from today I posted about my then-...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't know if this is allowed, I work in youth...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So I think I just ascended to a newer level of...</td>\n",
       "      <td>So I was rebattling the elite four in Pokemon ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>My friend got a gf before me</td>\n",
       "      <td>But I have more reddit karma so I think I win üòé</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.564514Z",
     "start_time": "2024-12-20T15:42:27.550292Z"
    }
   },
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "6678a56ff87dced0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "body     2074\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.611581Z",
     "start_time": "2024-12-20T15:42:27.597058Z"
    }
   },
   "cell_type": "code",
   "source": "df['text'] = df['title'] + '. ' + df['body']",
   "id": "58269d62baf0e2b1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.658100Z",
     "start_time": "2024-12-20T15:42:27.648396Z"
    }
   },
   "cell_type": "code",
   "source": "df.loc[df['body'].isna(),'text'] = df.loc[df['body'].isna(),'title']",
   "id": "7d15721ae4b4d609",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T15:42:27.705421Z",
     "start_time": "2024-12-20T15:42:27.697727Z"
    }
   },
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "74ce9b62c564a843",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title       0\n",
       "body     2074\n",
       "label       0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:27:48.529480Z",
     "start_time": "2024-12-20T16:27:48.503871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Dataset preparation\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = self.tokenizer(text, self.max_length)\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Simple tokenizer using built vocabulary\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "    def __call__(self, text, max_length):\n",
    "        tokens = self.tokenize(text)\n",
    "        return tokens[:max_length] + [0] * (max_length - len(tokens))  # Pad or truncate\n",
    "\n",
    "# 2. Vocabulary building\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        word_counter.update(text.split())\n",
    "    vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, count in word_counter.items() if count >= min_freq]\n",
    "    return vocab\n",
    "\n",
    "# 3. Model definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, num_classes, max_length, ff_dim, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Attention Pooling\n",
    "        self.attention_weights = nn.Linear(embed_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Attention Pooling\n",
    "        attention_scores = self.softmax(self.attention_weights(x))  # [batch_size, seq_len, 1]\n",
    "        x = (attention_scores * x).sum(dim=1)  # Weighted sum -> [batch_size, embed_size]\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# 4. Training loop\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.sigmoid(outputs).round()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_auc = roc_auc_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "def train_model_with_threshold(model, train_loader, val_loader, tokenizer, max_length, epochs, device, optimizer, criterion):\n",
    "    model.to(device)\n",
    "    best_threshold = 0.5  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5)\n",
    "    best_f1 = 0  # –õ—É—á—à–∏–π F1-score\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ======== –û–ë–£–ß–ï–ù–ò–ï ========\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ======== –û–¶–ï–ù–ö–ê –ù–ê –í–ê–õ–ò–î–ê–¶–ò–ò ========\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "                val_labels.extend(labels.numpy())\n",
    "                val_probs.extend(probs)\n",
    "\n",
    "        # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "        thresholds = precision_recall_curve(val_labels, val_probs)[2]  # –ü–æ—Ä–æ–≥–∏\n",
    "        f1_scores = [f1_score(val_labels, (val_probs >= t).astype(int)) for t in thresholds]\n",
    "        epoch_best_threshold = thresholds[f1_scores.index(max(f1_scores))]\n",
    "        epoch_best_f1 = max(f1_scores)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –ª—É—á—à–µ\n",
    "        if epoch_best_f1 > best_f1:\n",
    "            best_f1 = epoch_best_f1\n",
    "            best_threshold = epoch_best_threshold\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Best F1: {best_f1:.4f} at threshold {best_threshold:.4f}\")\n",
    "\n",
    "    return model, best_threshold"
   ],
   "id": "d111e4d934ea157e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:27:48.835719Z",
     "start_time": "2024-12-20T16:27:48.830172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    # ROC-–∫—Ä–∏–≤–∞—è\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_true, y_probs)\n",
    "\n",
    "    # Precision-Recall –∫—Ä–∏–≤–∞—è\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "    # F1-—Å–∫–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "    f1_scores = [f1_score(y_true, (y_probs >= thresh).astype(int)) for thresh in thresholds_pr]\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥ –ø–æ F1-score\n",
    "    best_threshold = thresholds_pr[f1_scores.index(max(f1_scores))]\n",
    "    best_f1 = max(f1_scores)\n",
    "\n",
    "    print(f\"–õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold:.4f}, F1-score: {best_f1:.4f}\")\n",
    "    return best_threshold\n"
   ],
   "id": "2962d74b86c14909",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:27:49.146936Z",
     "start_time": "2024-12-20T16:27:49.142318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, processors\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.decoders import WordPiece\n",
    "\n",
    "# 1. –°–æ–∑–¥–∞–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=5000):\n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.tokenizer.normalizer = Lowercase()\n",
    "        self.tokenizer.decoder = WordPiece()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def train(self, texts):\n",
    "        trainer = trainers.BpeTrainer(vocab_size=self.vocab_size, special_tokens=[\"<PAD>\", \"<UNK>\"])\n",
    "        self.tokenizer.train_from_iterator(texts, trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.encode(text).ids\n",
    "\n",
    "    def __call__(self, text, max_length):\n",
    "        tokens = self.tokenize(text)\n",
    "        return tokens[:max_length] + [0] * (max_length - len(tokens))  # Pad or truncate\n",
    "\n",
    "# 2. –û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "def build_bpe_tokenizer(texts, vocab_size=5000):\n",
    "    tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
    "    tokenizer.train(texts)\n",
    "    return tokenizer"
   ],
   "id": "4eb2deb05ad0ac3d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:35:32.980138Z",
     "start_time": "2024-12-20T16:27:49.984785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Build vocab from df['text']\n",
    "# vocab = build_vocab(df['text'], min_freq=1)\n",
    "# print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Hyperparameters\n",
    "max_length = 50\n",
    "embed_size = 128\n",
    "num_heads = 4\n",
    "num_layers = 8\n",
    "ff_dim = 256\n",
    "num_classes = 1\n",
    "vocab_size = 5000\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = build_bpe_tokenizer(df['text'].tolist(), vocab_size=vocab_size)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "texts, labels = df['text'].tolist(), df['label'].tolist()\n",
    "dataset = RedditDataset(texts, labels, tokenizer, max_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model, optimizer, loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerClassifier(vocab_size, embed_size, num_heads, num_layers, num_classes, max_length, ff_dim=ff_dim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the model\n",
    "# train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device)\n",
    "trained_model, best_threshold = train_model_with_threshold(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    epochs=15,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ],
   "id": "fd90863cb6f45197",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 0.4353 | Best F1: 0.5245 at threshold 0.4443\n",
      "Epoch 2/15 | Loss: 0.0587 | Best F1: 0.5551 at threshold 0.3812\n",
      "Epoch 3/15 | Loss: 0.0863 | Best F1: 0.5723 at threshold 0.2145\n",
      "Epoch 4/15 | Loss: 0.3728 | Best F1: 0.5882 at threshold 0.3466\n",
      "Epoch 5/15 | Loss: 0.1510 | Best F1: 0.5882 at threshold 0.3466\n",
      "Epoch 6/15 | Loss: 0.1438 | Best F1: 0.6109 at threshold 0.2463\n",
      "Epoch 7/15 | Loss: 0.0610 | Best F1: 0.6109 at threshold 0.2463\n",
      "Epoch 8/15 | Loss: 0.2977 | Best F1: 0.6109 at threshold 0.2463\n",
      "Epoch 9/15 | Loss: 0.1156 | Best F1: 0.6149 at threshold 0.2256\n",
      "Epoch 10/15 | Loss: 0.0425 | Best F1: 0.6311 at threshold 0.4222\n",
      "Epoch 11/15 | Loss: 0.0242 | Best F1: 0.6311 at threshold 0.4222\n",
      "Epoch 12/15 | Loss: 0.2553 | Best F1: 0.6311 at threshold 0.4222\n",
      "Epoch 13/15 | Loss: 0.2108 | Best F1: 0.6311 at threshold 0.4222\n",
      "Epoch 14/15 | Loss: 0.1172 | Best F1: 0.6530 at threshold 0.3003\n",
      "Epoch 15/15 | Loss: 0.6056 | Best F1: 0.6599 at threshold 0.3773\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T17:04:52.924591Z",
     "start_time": "2024-12-20T16:57:12.706505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_model, best_threshold = train_model_with_threshold(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    epochs=15,\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion\n",
    ")"
   ],
   "id": "59436ae5c1181600",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 0.1478 | Best F1: 0.7551 at threshold 0.5955\n",
      "Epoch 2/15 | Loss: 0.1019 | Best F1: 0.7551 at threshold 0.5955\n",
      "Epoch 3/15 | Loss: 0.0755 | Best F1: 0.7618 at threshold 0.5402\n",
      "Epoch 4/15 | Loss: 0.1003 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 5/15 | Loss: 0.0032 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 6/15 | Loss: 0.0048 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 7/15 | Loss: 0.0051 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 8/15 | Loss: 0.5743 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 9/15 | Loss: 0.9573 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 10/15 | Loss: 0.0291 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 11/15 | Loss: 0.5175 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 12/15 | Loss: 0.0369 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 13/15 | Loss: 0.0950 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 14/15 | Loss: 0.1882 | Best F1: 0.7680 at threshold 0.5858\n",
      "Epoch 15/15 | Loss: 0.1469 | Best F1: 0.7680 at threshold 0.5858\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T17:12:29.548402Z",
     "start_time": "2024-12-20T17:12:28.544064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "def predict_on_test(model, test_texts, tokenizer, max_length, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    test_tokens = [tokenizer(text, max_length) for text in test_texts]\n",
    "    test_dataset = torch.tensor(test_tokens, dtype=torch.long)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)\n",
    "            probs = torch.sigmoid(outputs).squeeze(dim=-1)  # Ensure it's a 1D array\n",
    "            predictions.extend(probs.cpu().numpy().tolist())  # Convert to list and extend\n",
    "    return predictions\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ –ø—É—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π)\n",
    "test_df = pd.read_csv('./depr/test.csv')\n",
    "test_df['text'] = test_df['title'] + '. ' + test_df['body']\n",
    "test_df.loc[test_df['body'].isna(),'text'] = test_df.loc[test_df['body'].isna(),'title']\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "test_predictions = predict_on_test(model, test_df['text'].tolist(), tokenizer, max_length, device)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ DataFrame\n",
    "test_df['label'] = test_predictions\n",
    "test_df['label'] = (np.array(test_predictions) >= best_threshold).astype(int)\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "test_df[['id', 'label']].to_csv(\"depr/test_predictions.csv\", index=False)\n",
    "print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_predictions.csv\")\n"
   ],
   "id": "e2df3577600a8492",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:03:46.929596Z",
     "start_time": "2024-12-20T16:03:45.803469Z"
    }
   },
   "cell_type": "code",
   "source": "best_threshold = find_best_threshold(df['text'].tolist(), df['label'].tolist())",
   "id": "e43401a83d6cb8af",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m best_threshold \u001B[38;5;241m=\u001B[39m \u001B[43mfind_best_threshold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[21], line 5\u001B[0m, in \u001B[0;36mfind_best_threshold\u001B[1;34m(y_true, y_probs)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind_best_threshold\u001B[39m(y_true, y_probs):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# ROC-–∫—Ä–∏–≤–∞—è\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m     fpr, tpr, thresholds_roc \u001B[38;5;241m=\u001B[39m \u001B[43mroc_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_probs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# Precision-Recall –∫—Ä–∏–≤–∞—è\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     precision, recall, thresholds_pr \u001B[38;5;241m=\u001B[39m precision_recall_curve(y_true, y_probs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1145\u001B[0m, in \u001B[0;36mroc_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001B[0m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1044\u001B[0m     {\n\u001B[0;32m   1045\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1054\u001B[0m     y_true, y_score, \u001B[38;5;241m*\u001B[39m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, drop_intermediate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1055\u001B[0m ):\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m \n\u001B[0;32m   1058\u001B[0m \u001B[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1143\u001B[0m \u001B[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001B[39;00m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1145\u001B[0m     fps, tps, thresholds \u001B[38;5;241m=\u001B[39m \u001B[43m_binary_clf_curve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[0;32m   1147\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001B[39;00m\n\u001B[0;32m   1150\u001B[0m     \u001B[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001B[39;00m\n\u001B[0;32m   1151\u001B[0m     \u001B[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001B[39;00m\n\u001B[0;32m   1157\u001B[0m     \u001B[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_intermediate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fps) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:817\u001B[0m, in \u001B[0;36m_binary_clf_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight)\u001B[0m\n\u001B[0;32m    815\u001B[0m y_type \u001B[38;5;241m=\u001B[39m type_of_target(y_true, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m pos_label \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m--> 817\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m format is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(y_type))\n\u001B[0;32m    819\u001B[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001B[0;32m    820\u001B[0m y_true \u001B[38;5;241m=\u001B[39m column_or_1d(y_true)\n",
      "\u001B[1;31mValueError\u001B[0m: multiclass format is not supported"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T16:05:31.266428Z",
     "start_time": "2024-12-20T16:05:31.161153Z"
    }
   },
   "cell_type": "code",
   "source": "pd.Series(test_predictions).hist()",
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T18:30:39.517008Z",
     "start_time": "2024-12-20T18:30:39.488967Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"depr/model.pt\")",
   "id": "20ded9633cc0e71b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57a6c0eebc24295f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
