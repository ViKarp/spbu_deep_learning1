{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eda323-3063-435f-bc1a-b1446b014c1c",
   "metadata": {},
   "source": [
    "# Простейшая рекуррентная сеть\n",
    "В этом ноутбуке мы пройдемся по основам работы с RNN. Сегодня займемся задачей генерации текста. "
   ]
  },
  {
   "cell_type": "code",
   "id": "70d8b089-5f9c-4dcb-8b14-3f565c24e438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:33.677689Z",
     "start_time": "2024-12-01T12:37:32.327974Z"
    }
   },
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "198424b3-07c0-4b46-83f0-8bbb53acacd4",
   "metadata": {},
   "source": [
    "В качестве обучающего датасета возьмем набор из 120 тысяч анекдотов на русском языке. \n",
    "[Ссылка на данные](https://archive.org/download/120_tysyach_anekdotov) и [пост на хабре про тематическое моделирование](https://habr.com/ru/companies/otus/articles/723306/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5fda8b3-2e4b-4385-aad5-b10ad73a5d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:33.770881Z",
     "start_time": "2024-12-01T12:37:33.684203Z"
    }
   },
   "source": [
    "with open(r\"../additional_materials/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[118:500]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "007f21a5-c7e3-445f-b902-24e18242bd7b",
   "metadata": {},
   "source": [
    "Мы не хотим моделировать все подряд, поэтому разобьем датасет на отдельные анекдоты.  "
   ]
  },
  {
   "cell_type": "code",
   "id": "fddd3f65-a156-4bbd-8c56-078652d38ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.205721Z",
     "start_time": "2024-12-01T12:37:34.190687Z"
    }
   },
   "source": [
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3ae42013-ef71-485c-805e-8cc4c61fe6f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.268416Z",
     "start_time": "2024-12-01T12:37:34.222236Z"
    }
   },
   "source": [
    "cut_text = cut_data(text)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "67e8e214-e40c-4705-beb4-f51a6a284137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.299962Z",
     "start_time": "2024-12-01T12:37:34.285447Z"
    }
   },
   "source": [
    "cut_text[1:6]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!',\n",
       " '- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...',\n",
       " '- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.',\n",
       " 'Поссорилась с мужем. Пока он спал, я мысленно развелась с ним, поделила имущество, переехала, поняла, что жить без него не могу, дала последний шанс, вернулась. В итоге, ложусь спать уже счастливой женщиной.',\n",
       " 'Если тебя посещают мысли о смерти - это еще полбеды. Беда - это когда смерть посещают мысли о тебе...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "282f6226-74c6-4488-a7bc-9360437e1b1f",
   "metadata": {},
   "source": [
    "Сделаем для начала самую простую модель с токенами на уровне символов. Это значит, что каждому символу в тексте ставится в соответствие некоторое число. Некоторые способы токенизации используют части слов или, наоборот, части бинарного представления текста."
   ]
  },
  {
   "cell_type": "code",
   "id": "3e923efb-a3d5-4e22-b8e0-8bf6260d1e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.687441Z",
     "start_time": "2024-12-01T12:37:34.316991Z"
    }
   },
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "f99fa447-208d-4285-bb76-0870e985ce58",
   "metadata": {},
   "source": [
    "Напишем функции для энкодинга и декодинга нашего текста. Они будут преобразовывать список символов в список чисел и обратно."
   ]
  },
  {
   "cell_type": "code",
   "id": "97704441-98c6-4c16-b0c2-10b88f941c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.718971Z",
     "start_time": "2024-12-01T12:37:34.704462Z"
    }
   },
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab[sys] for sys in sentence] # List of ints \n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    return [vocab[toc] for toc in tokens]# list of strings"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "9d0b4340-44bb-45ab-8cfe-972c9cfd410e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.749597Z",
     "start_time": "2024-12-01T12:37:34.735004Z"
    }
   },
   "source": "encode(cut_text[0], char2int)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[208,\n",
       " 78,\n",
       " 106,\n",
       " 123,\n",
       " 80,\n",
       " 78,\n",
       " 122,\n",
       " 111,\n",
       " 149,\n",
       " 71,\n",
       " 142,\n",
       " 144,\n",
       " 197,\n",
       " 106,\n",
       " 27,\n",
       " 122,\n",
       " 12,\n",
       " 144,\n",
       " 78,\n",
       " 122,\n",
       " 14,\n",
       " 106,\n",
       " 78,\n",
       " 16,\n",
       " 78,\n",
       " 122,\n",
       " 82,\n",
       " 105,\n",
       " 69,\n",
       " 191,\n",
       " 143,\n",
       " 78,\n",
       " 82,\n",
       " 122,\n",
       " 143,\n",
       " 149,\n",
       " 94,\n",
       " 197,\n",
       " 191,\n",
       " 149,\n",
       " 142,\n",
       " 144,\n",
       " 14,\n",
       " 98,\n",
       " 122,\n",
       " 14,\n",
       " 149,\n",
       " 71,\n",
       " 197,\n",
       " 71,\n",
       " 197,\n",
       " 122,\n",
       " 9,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 191,\n",
       " 149,\n",
       " 106,\n",
       " 123,\n",
       " 143,\n",
       " 207,\n",
       " 71,\n",
       " 197,\n",
       " 122,\n",
       " 80,\n",
       " 106,\n",
       " 149,\n",
       " 16,\n",
       " 197,\n",
       " 140,\n",
       " 149,\n",
       " 71,\n",
       " 197,\n",
       " 35,\n",
       " 122,\n",
       " 91,\n",
       " 149,\n",
       " 80,\n",
       " 122,\n",
       " 16,\n",
       " 14,\n",
       " 142,\n",
       " 122,\n",
       " 105,\n",
       " 191,\n",
       " 78,\n",
       " 65,\n",
       " 204,\n",
       " 71,\n",
       " 149,\n",
       " 143,\n",
       " 78,\n",
       " 27,\n",
       " 122,\n",
       " 94,\n",
       " 106,\n",
       " 197,\n",
       " 143,\n",
       " 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "017baeba-1197-4d21-8cc8-28ccf43262c5",
   "metadata": {},
   "source": [
    "Просто представления символов в виде числа не подходят для обучения моделей. На выходе должны быть вероятности всех возможных токенов из словаря. Поэтому модели удобно учить с помощью энтропии. К тому же, токены часто преобразуют из исходного представления в эмбеддинги, которые также позволяют получить более удобное представление в высокоразмерном пространстве. \n",
    "\n",
    "В итоге векторы в модели выглядят следующим образом:\n",
    "![alt_text](../additional_materials/images/char_rnn.jfif)\n",
    "\n",
    "Задание: реализуйте метод, который преобразует батч в бинарное представление."
   ]
  },
  {
   "cell_type": "code",
   "id": "e692112f-edea-4e18-b48b-5aec75f935d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:34.827813Z",
     "start_time": "2024-12-01T12:37:34.817237Z"
    }
   },
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encodes a batch of sentences (integer indices) into binary one-hot representation.\n",
    "    \n",
    "    Args:\n",
    "        int_words (torch.Tensor): Tensor of size (batch_size, seq_len) containing word indices.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded tensor of size (batch_size, seq_len, vocab_size).\n",
    "    \"\"\"\n",
    "    words_one_hot = torch.zeros(\n",
    "        (int_words.numel(), vocab_size), dtype=torch.float32, device=int_words.device\n",
    "    )\n",
    "    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten().long()] = 1.0\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, vocab_size))\n",
    "    return words_one_hot\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "88488683-6df3-430e-b942-9c10548a1802",
   "metadata": {},
   "source": [
    "Проверьте ваш код."
   ]
  },
  {
   "cell_type": "code",
   "id": "af941c64-cc6d-41b4-92e3-a8f37b861545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:35.248811Z",
     "start_time": "2024-12-01T12:37:35.235298Z"
    }
   },
   "source": [
    "test_seq = torch.tensor([[2, 6, 4, 1], [0,3, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "8da82134-e59d-4806-be2c-839c2f850ee6",
   "metadata": {},
   "source": [
    "Однако, наши последовательности на самом деле разной длины. Как же объединить их в батч?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe1e-40a5-4a58-b1bd-b4a4101d986a",
   "metadata": {},
   "source": [
    "Реализуем два необходимых класса: \n",
    "- токенайзер, который будет брать текст, кодировать и декодировать символы. Еще одно, что будет реализовано там - добавлено несколько специальных символов (паддинг, конец последовательности, начало последовательности).\n",
    "- Датасет, который будет брать набор шуток, используя токенайзер, строить эмбеддинги и дополнять последовательность до максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "id": "69d266c8-b4d0-42fd-9c6c-02b7f3b9a4bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:36.167853Z",
     "start_time": "2024-12-01T12:37:36.153288Z"
    }
   },
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, cut_text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(tuple(set(text))))\n",
    "        self.char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "        self._add_special(\"<pad>\")\n",
    "        self._add_special('<bos>')\n",
    "        self._add_special('<eos>')\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.char2int)\n",
    "        self.char2int[symbol] = sym_num\n",
    "        self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2char) # your code\n",
    "        \n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int[el]\n",
    "        \n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.char2int[sym] for sym in chars] # str -> list[int]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.int2char[toc] for toc in idx] # list[int] -> list[str]\n",
    "\n",
    "    def encode(self, chars):\n",
    "        chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars) # make string from list"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "b8a76399-0ae4-4d4f-9d95-56d695eb7f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:43:21.159350Z",
     "start_time": "2024-12-01T16:43:21.131409Z"
    }
   },
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode(\"<pad>\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.cut_text[idx]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded = encoded[:self.max_len]  # Ограничиваем длину\n",
    "        input_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        target_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        \n",
    "        # Заполняем входную и целевую последовательность\n",
    "        input_sequence[:len(encoded) - 1] = torch.tensor(encoded[:-1])\n",
    "        target_sequence[1:len(encoded)] = torch.tensor(encoded[1:])\n",
    "        \n",
    "        return input_sequence, target_sequence"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "af9e66a2-d196-459f-a88a-94bc119873e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:38.338781Z",
     "start_time": "2024-12-01T12:37:37.623283Z"
    }
   },
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "2c72173d-d38b-4d4c-a98e-878267c0fd87",
   "metadata": {},
   "source": [
    "Вопрос: А как бы мы должны были разделять данные на последовательности и батчи в случае, если бы использовался сплошной текст?"
   ]
  },
  {
   "cell_type": "code",
   "id": "182387e9-9768-42b2-b428-16d73b24b07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:38.415316Z",
     "start_time": "2024-12-01T12:37:38.402792Z"
    }
   },
   "source": [
    "for batch in dataloader:\n",
    "    break\n",
    "batch[1].shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a9bf1f16-53d0-45a6-abd5-1a4c5b17285f",
   "metadata": {},
   "source": [
    "Теперь реализуем нашу модель. \n",
    "Необходимо следующее:\n",
    " - Используя токенайзер, задать размер словаря\n",
    " - Задать слой RNN с помощью torch.RNN. Доп.задание: создайте модель, используя слой LSTM.\n",
    " - Задать полносвязный слой с набором параметров: размерность ввода — n_hidden; размерность выхода — размер словаря. Этот слой преобразует состояние модели в логиты токенов.\n",
    " - Определить шаг forward, который будет использоваться при обучении\n",
    " - Определить метод init_hidden, который будет задавать начальное внутреннее состояние. Инициализировать будем нулями.\n",
    " - Определить метод inference, в котором будет происходить генерация последовательности из префикса. Здесь мы уже не используем явные логиты, а семплируем токены на их основе.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8cc03daf-9a78-4d34-a8da-c1aed594b60a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:39.036729Z",
     "start_time": "2024-12-01T12:37:39.018699Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from typing import Tuple\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Токенизатор для кодирования и декодирования\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size # размер словаря\n",
    "        \n",
    "        # RNN (или LSTM) слой\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.vocab_size,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # Dropout для регуляризации\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # Полносвязный слой: преобразует состояние RNN в логиты\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # One-hot кодирование входной последовательности\n",
    "        x = one_hot_encode(x, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Упаковка последовательностей для эффективности\n",
    "        packed_embeds = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Прогон через LSTM\n",
    "        packed_outputs, hidden = self.rnn(packed_embeds)\n",
    "        \n",
    "        # Распаковка выхода обратно в тензор\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        \n",
    "        # Dropout для регуляризации\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        # Преобразование выхода RNN в логиты\n",
    "        logits = self.fc(outputs)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int, device: str = \"cpu\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Инициализация начального скрытого состояния нулями\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return h0, c0\n",
    "\n",
    "    def inference(self, prefix=\"<bos> \", device=\"cpu\"):\n",
    "        # Кодирование начального префикса\n",
    "        tokens = torch.tensor(self.tokenizer.encode(prefix), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Создание one-hot представления\n",
    "        inputs = one_hot_encode(tokens, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Инициализация скрытого состояния\n",
    "        hidden = self.init_hidden(batch_size=1, device=device)\n",
    "        \n",
    "        # Генерация префикса\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        logits = self.fc(outputs)\n",
    "        \n",
    "        # Семплирование токена\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "        new_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Остановка: достижение максимальной длины или EOS-токена\n",
    "        while tokens.size(1) < self.max_len and new_token.item() != self.tokenizer.encode('<eos>'):\n",
    "            inputs = one_hot_encode(new_token, vocab_size=self.vocab_size)\n",
    "            outputs, hidden = self.rnn(inputs, hidden)\n",
    "            logits = self.fc(outputs)\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Декодирование в строку\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "1202bfda-3653-4644-8fcc-eda9e92e434f",
   "metadata": {},
   "source": [
    "Зададим параметры для обучения. Можете варьировать их, чтобы вам хватило ресурсов."
   ]
  },
  {
   "cell_type": "code",
   "id": "173284d2-1d28-4235-a3ac-e25494039e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:40.568082Z",
     "start_time": "2024-12-01T12:37:40.554521Z"
    }
   },
   "source": [
    "batch_size = 16\n",
    "seq_length = 512\n",
    "n_hidden = 128\n",
    "n_layers = 6\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "8329823d-abd8-4044-8206-470f07b6da62",
   "metadata": {},
   "source": [
    "Напишите функцию для одного тренировочного шага. В этом ноутбуке сам процесс обучения модели достаточно тривиален, поэтому мы не будем использовать сложные функции для обучающего цикла. Вы же, однако, можете дописать их."
   ]
  },
  {
   "cell_type": "code",
   "id": "737cb10c-1da8-43fc-b332-1b53f4fb6e09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:37:41.606475Z",
     "start_time": "2024-12-01T12:37:41.600157Z"
    }
   },
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    # Обнуляем градиенты\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Извлекаем данные из пакета\n",
    "    inputs, targets = train_batch\n",
    "    batch_size, seq_len = inputs.shape\n",
    "\n",
    "    # Переносим данные на нужное устройство (например, GPU)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Прямой проход через модель\n",
    "    lengths = (inputs != 0).sum(dim=1)  # или другая логика для определения длин\n",
    "    logits, _ = model(inputs, lengths)  # Получаем логиты от модели\n",
    "\n",
    "    # Переходим от логитов к потере\n",
    "    # targets нужно сдвигать на 1, чтобы правильно сравнить предсказания и настоящие метки\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Обновление весов\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "d1055e6e-6374-4af3-b1ab-8ad8b4125664",
   "metadata": {},
   "source": [
    "Инициализируйте модель, функцию потерь и оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "id": "f85fc024-7cec-4833-ac15-cafe05724003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:43:46.584239Z",
     "start_time": "2024-12-01T12:43:46.560356Z"
    }
   },
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "a140de0c-e648-4d9f-babf-659486dbae92",
   "metadata": {},
   "source": [
    "Проверьте необученную модель: она должна выдавать бессмысленные последовательности"
   ]
  },
  {
   "cell_type": "code",
   "id": "80d26fdb-5292-41c4-83df-e8feb3a22561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:43:47.876113Z",
     "start_time": "2024-12-01T12:43:47.779976Z"
    }
   },
   "source": [
    "model.eval()  # Переключаем модель в режим оценки (inference)\n",
    "\n",
    "# Шаг 3: Генерация текста\n",
    "prefix = \"<bos> \"  # Начальный токен последовательности\n",
    "generated_sequence = model.inference(prefix=prefix, device=\"cuda\")\n",
    "\n",
    "# Шаг 4: Вывод результата\n",
    "print(\"Сгенерированная последовательность необученной моделью:\")\n",
    "print(generated_sequence)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированная последовательность необученной моделью:\n",
      "<bos><bos> <eos>СЖ事☺老人新AπМhаЦ́的fj<eos>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "37263cdf-5a6c-4612-8cf9-57105c169943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:43:48.062270Z",
     "start_time": "2024-12-01T12:43:48.044879Z"
    }
   },
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "44f70324-c88a-4d98-bae6-e6c5356f2025",
   "metadata": {},
   "source": [
    "Проведите обучение на протяжении нескольких эпох и выведите график лоссов."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:43:48.542026Z",
     "start_time": "2024-12-01T12:43:48.523247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for train_batch in dataloader:\n",
    "    inputs, targets = train_batch\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Targets shape: {targets.shape}\")\n",
    "    print(targets)\n",
    "    break"
   ],
   "id": "6ebeea1a4cdb8e75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([16, 512])\n",
      "Targets shape: torch.Size([16, 512])\n",
      "tensor([[214,  91, 191,  ..., 214, 214, 214],\n",
      "        [214,  20, 149,  ..., 214, 214, 214],\n",
      "        [214,  20, 149,  ..., 214, 214, 214],\n",
      "        ...,\n",
      "        [214, 187, 149,  ..., 214, 214, 214],\n",
      "        [214,  20, 142,  ..., 214, 214, 214],\n",
      "        [214, 151,  14,  ..., 214, 214, 214]])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:43:48.823543Z",
     "start_time": "2024-12-01T12:43:48.809985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch_idx, train_batch in enumerate(dataloader):\n",
    "    inputs, targets = train_batch\n",
    "    print(f\"Batch {batch_idx}: Inputs shape = {inputs.shape}, Targets shape = {targets.shape}\")\n",
    "    break"
   ],
   "id": "eb2084ae344e7020",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Inputs shape = torch.Size([16, 512]), Targets shape = torch.Size([16, 512])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "57059744-44ac-4aad-86f6-f67dc2ea7600",
   "metadata": {},
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "# Основной цикл обучения\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0  # Суммарные потери за эпоху\n",
    "    model.train()  # Переключение в режим тренировки\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):  # train_loader — DataLoader с батчами\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cuda')\n",
    "        losses.append(loss.item())  # Запись потерь\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Логгирование каждые 100 батчей\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx + 1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Сохранение весов после каждой эпохи\n",
    "    torch.save(model.state_dict(), f\"rnn_epoch_{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Визуализация потерь\n",
    "    plot_losses(losses)\n",
    "\n",
    "# Финальное сохранение модели\n",
    "torch.save(model.state_dict(), \"rnn_final.pt\")\n",
    "print(\"Training completed and model saved.\")"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [100], Loss: 0.8150\n",
      "Epoch [2/5], Step [200], Loss: 1.0294\n",
      "Epoch [2/5], Step [300], Loss: 1.1431\n",
      "Epoch [2/5], Step [400], Loss: 0.9548\n",
      "Epoch [2/5], Step [500], Loss: 1.0173\n",
      "Epoch [2/5], Step [600], Loss: 0.7895\n",
      "Epoch [2/5], Step [700], Loss: 0.9674\n",
      "Epoch [2/5], Step [800], Loss: 1.0423\n",
      "Epoch [2/5], Step [900], Loss: 0.9236\n",
      "Epoch [2/5], Step [1000], Loss: 1.0404\n",
      "Epoch [2/5], Step [1100], Loss: 0.9618\n",
      "Epoch [2/5], Step [1200], Loss: 0.9906\n",
      "Epoch [2/5], Step [1300], Loss: 0.9256\n",
      "Epoch [2/5], Step [1400], Loss: 1.1013\n",
      "Epoch [2/5], Step [1500], Loss: 1.0647\n",
      "Epoch [2/5], Step [1600], Loss: 0.9376\n",
      "Epoch [2/5], Step [1700], Loss: 0.8398\n",
      "Epoch [2/5], Step [1800], Loss: 0.9222\n",
      "Epoch [2/5], Step [1900], Loss: 1.0000\n",
      "Epoch [2/5], Step [2000], Loss: 0.8437\n",
      "Epoch [2/5], Step [2100], Loss: 0.8996\n",
      "Epoch [2/5], Step [2200], Loss: 0.9899\n",
      "Epoch [2/5], Step [2300], Loss: 1.1194\n",
      "Epoch [2/5], Step [2400], Loss: 1.1276\n",
      "Epoch [2/5], Step [2500], Loss: 0.8143\n",
      "Epoch [2/5], Step [2600], Loss: 0.8764\n",
      "Epoch [2/5], Step [2700], Loss: 1.0707\n",
      "Epoch [2/5], Step [2800], Loss: 0.8774\n",
      "Epoch [2/5], Step [2900], Loss: 1.0138\n",
      "Epoch [2/5], Step [3000], Loss: 0.8611\n",
      "Epoch [2/5], Step [3100], Loss: 0.8517\n",
      "Epoch [2/5], Step [3200], Loss: 0.9128\n",
      "Epoch [2/5], Step [3300], Loss: 0.8790\n",
      "Epoch [2/5], Step [3400], Loss: 1.2264\n",
      "Epoch [2/5], Step [3500], Loss: 0.9197\n",
      "Epoch [2/5], Step [3600], Loss: 0.9194\n",
      "Epoch [2/5], Step [3700], Loss: 0.9594\n",
      "Epoch [2/5], Step [3800], Loss: 0.9420\n",
      "Epoch [2/5], Step [3900], Loss: 0.9092\n",
      "Epoch [2/5], Step [4000], Loss: 0.7548\n",
      "Epoch [2/5], Step [4100], Loss: 1.0040\n",
      "Epoch [2/5], Step [4200], Loss: 0.9795\n",
      "Epoch [2/5], Step [4300], Loss: 0.9304\n",
      "Epoch [2/5], Step [4400], Loss: 0.9570\n",
      "Epoch [2/5], Step [4500], Loss: 0.6958\n",
      "Epoch [2/5], Step [4600], Loss: 1.0455\n",
      "Epoch [2/5], Step [4700], Loss: 0.6867\n",
      "Epoch [2/5], Step [4800], Loss: 0.9193\n",
      "Epoch [2/5], Step [4900], Loss: 1.0012\n",
      "Epoch [2/5], Step [5000], Loss: 1.0943\n",
      "Epoch [2/5], Step [5100], Loss: 0.7868\n",
      "Epoch [2/5], Step [5200], Loss: 1.0312\n",
      "Epoch [2/5], Step [5300], Loss: 1.1125\n",
      "Epoch [2/5], Step [5400], Loss: 0.9237\n",
      "Epoch [2/5], Step [5500], Loss: 0.9123\n",
      "Epoch [2/5], Step [5600], Loss: 0.8913\n",
      "Epoch [2/5], Step [5700], Loss: 1.1143\n",
      "Epoch [2/5], Step [5800], Loss: 1.0115\n",
      "Epoch [2/5], Step [5900], Loss: 1.0947\n",
      "Epoch [2/5], Step [6000], Loss: 0.8220\n",
      "Epoch [2/5], Step [6100], Loss: 0.9092\n",
      "Epoch [2/5], Step [6200], Loss: 0.8735\n",
      "Epoch [2/5], Step [6300], Loss: 1.1585\n",
      "Epoch [2/5], Step [6400], Loss: 0.8809\n",
      "Epoch [2/5], Step [6500], Loss: 1.0103\n",
      "Epoch [2/5], Step [6600], Loss: 0.9200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Переключение в режим тренировки\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, train_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):  \u001B[38;5;66;03m# train_loader — DataLoader с батчами\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())  \u001B[38;5;66;03m# Запись потерь\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "Cell \u001B[1;32mIn[17], line 28\u001B[0m, in \u001B[0;36mtraining_step\u001B[1;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001B[0m\n\u001B[0;32m     25\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, vocab_size), targets\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Обратный проход\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Обновление весов\u001B[39;00m\n\u001B[0;32m     31\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c54494af74144b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T12:49:14.081263Z",
     "start_time": "2024-11-29T12:49:14.074048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "model.load_state_dict(torch.load('rnn_epoch_1.pt'))"
   ],
   "id": "753d563a4a2a567a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "72d8694a-132f-4a44-a5d3-a0f39219e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:09:14.728409Z",
     "start_time": "2024-12-01T13:09:12.696976Z"
    }
   },
   "source": "[model.inference(\"<bos>\", device='cuda') for _ in range(10)]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos><bos><eos>м-!<pad>д<pad>нооедтирнюов н-ьн<pad>ира! <pad>р<pad>к<pad>   <pad><pad>т<pad>оеи зое <pad> йщ те-с тсти<pad>а<pad>п л <pad>о чпо<pad>ж<pad>  ж<pad>лшу  поав<pad>а<pad><pad><pad><pad><pad>л<pad><pad>п<pad>п а<pad>км<pad><pad>  <pad><pad><pad><pad>,л<pad><pad>еа<pad><pad>т<pad><pad><pad><pad>н<pad>Се<pad><pad>т<pad><pad><pad><pad><pad><pad><pad><pad><pad>я<pad>й<pad><pad><pad>л<pad><pad><pad><pad><pad>т<pad><pad>т<pad><pad>а<pad><pad><pad>оа<pad><pad><pad><pad><pad>тр<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>р<pad><pad><pad><pad><pad><pad><pad><pad><pad>у<pad><pad><pad><pad><pad><pad><pad><pad>в<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>',\n",
       " '<bos><bos><eos> <pad> нзк  птп  оЕаьд<pad>аП,лотентатт<pad>ди еаабняли<pad><pad>еамр <pad>т<pad><pad>е ?о<pad>кннб,то втитхшдда оа<pad><pad><pad>х<pad>е<pad>а<pad>есМ <pad><pad>ос<pad>ли  -п <pad>е<pad><pad><pad>о<pad>С<eos>',\n",
       " '<bos><bos><eos>лорндрен<pad>.нй<pad>чо мдошкянгАодлемаичеяи<pad><pad>Фк<pad>ж ы<pad>н - <pad> рев<pad>о<pad> ш<pad>ыаснк  <pad>арчбтч <pad> <pad>л<pad> <pad>  пн:<pad><pad>ив<pad><pad>с<pad><pad>у<pad><pad>б<pad><pad><pad>п<pad><pad>кил тк <pad>оот<pad><pad>д<pad>-<pad><pad><pad>и<pad><pad><pad>ь<pad>.<pad><pad>пд<pad><pad><pad><pad><pad>е<pad>п<pad><pad><pad><pad>еи<pad><pad><pad><pad><pad>а <pad><pad><pad>д<pad><pad><pad><pad>ст<pad><pad><pad><pad><pad><pad><pad>в<pad>о<pad><pad><pad><pad><pad><pad><pad>н<pad><pad> е<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>!<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos><bos><eos><eos>',\n",
       " '<bos><bos><eos> ет ира<pad> жкнсп<pad>н на<pad> к  ьотбннтф<eos>',\n",
       " '<bos><bos><eos>к  оти<pad>о<pad>кд ркл<pad>н-,.е к:содмж <pad> те   ао <pad>л<pad>ынал<pad>оаа вектйкиа -е тео<pad>в<pad>ыную<pad> <pad>ир<pad>ип<pad>д <pad>Е<pad>, <pad>в.а<pad>кл<pad>е в  <pad><pad>о<pad>а<pad><pad><pad>ь<pad><pad>е<pad>д<pad><pad>кеиоб<pad>щ<pad><pad><pad>и<pad>п<pad><pad><pad>еэ<pad>ил<pad>с<pad><pad><pad><pad>Н<pad><pad><pad><pad>е<pad>ф<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> <pad><pad><pad><pad><pad><pad><pad><pad><pad>я<pad><pad><pad><pad><pad>.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>т<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos><bos><eos> зо вун<pad>шзсгсрс<pad>лмдп-б<pad>иое!тОоонлрпН<pad> н<pad>и.ес  опд о еу<pad> ье<pad>.ж <pad>дслр<pad>ле<pad>ч<pad>оже<pad> а<pad> сБэ<pad>  пив<pad><pad> ин<pad>о<pad>т<pad>нас<pad>оте<pad>чл<pad>с<pad>зьчВр<pad><pad>аи<pad>кс<pad><pad><pad><pad><pad>юба<pad><pad>о<pad>с<pad>ла,<pad> ям<pad><pad><pad><pad><pad><pad><pad><pad>х<pad><pad><pad><pad>о<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>р<pad>о<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>е<pad> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos><bos><eos>п <pad>ем<pad>и<pad>ые а т<pad>а <pad>бщж ндкоов <pad>дйлюеп ап гп<pad>зкуаД<pad>па еоеяыф,<pad><pad>лгекдфдрр <pad><pad>уядет<pad>врь бк<pad><pad><pad><pad>дг<pad>вни<pad><pad>оо\"<pad><pad>лт<pad>о<pad>цвй<pad><pad>н  е<pad><pad><pad>а я<pad>з<pad>е ь<pad> н<pad><pad>т<pad>рн <pad><pad><pad>м<pad>ж<pad><pad> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>ш<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>ы<pad><pad><pad>о<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>н<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos><bos><eos><pad>асиншаямйоо бч а-юа, таея лй е<pad>с тклт<pad>л<pad>аалэН  -мвуооплча<pad>вто<pad>оювьед<pad><pad> <pad><pad><pad>увмо<pad>р члж<pad> <pad><pad>а<pad>рцдаа бьноыор<pad>а тэа<pad>т<pad>в<pad>р<pad>тг<pad>т<pad>я йр<pad>т<pad>э<pad><pad><pad><pad>а<pad><pad>л д.<pad><pad><pad><pad>е<pad><pad>Д<pad><pad><pad><pad><pad><pad><pad>кв<pad><pad> <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>х<pad><pad>и<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>у<pad><pad><pad><pad>й<pad><pad><pad><pad><pad>бо<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<bos><bos><eos>ивчакмои ц и   н<pad>отоаао?ювак д а  х<pad>к\"яэодшбрк,м озцсау<pad> аыо<pad> о <pad>.<pad>ыа оси<pad>ро<pad>вк<pad><pad><pad><pad><pad> <pad>ио <pad>оу.<pad>ея<pad>, <pad>саде<pad><pad>-<pad>м<eos>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "8f13c3b7-3fde-416b-a766-e6be7c791505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:44:32.971024Z",
     "start_time": "2024-12-01T16:44:32.944885Z"
    }
   },
   "source": [
    "import pymorphy2\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        # Уникальные символы в тексте\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(unique_chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        # Добавление специальных символов\n",
    "        for special in self.specials:\n",
    "            self._add_special(special)\n",
    "\n",
    "    def _add_special(self, symbol) -> None:\n",
    "        \"\"\"Добавить специальные символы в словари.\"\"\"\n",
    "        if symbol not in self.char2int:\n",
    "            sym_num = len(self.char2int)\n",
    "            self.char2int[symbol] = sym_num\n",
    "            self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Размер словаря.\"\"\"\n",
    "        return len(self.int2char)\n",
    "\n",
    "    def decode_symbol(self, el):\n",
    "        \"\"\"Декодировать символ по индексу.\"\"\"\n",
    "        return self.int2char[el]\n",
    "\n",
    "    def encode_symbol(self, el):\n",
    "        \"\"\"Закодировать символ в индекс.\"\"\"\n",
    "        return self.char2int[el]\n",
    "\n",
    "    def str_to_idx(self, chars):\n",
    "        \"\"\"Строку в индексы.\"\"\"\n",
    "        return [self.char2int[sym] for sym in chars]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        \"\"\"Индексы в строку.\"\"\"\n",
    "        return [self.int2char[toc] for toc in idx]\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Закодировать строку с добавлением спецсимволов.\"\"\"\n",
    "        lemmatized_text = self.lemmatize(text)\n",
    "        chars = ['<bos>'] + list(lemmatized_text) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        \"\"\"Декодировать индексы в строку.\"\"\"\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"Лемматизировать входной текст.\"\"\"\n",
    "        words = text.split()\n",
    "        lemmatized = \" \".join([self.morph.parse(word)[0].normal_form for word in words])\n",
    "        return lemmatized\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:44:33.776458Z",
     "start_time": "2024-12-01T16:44:33.697941Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = Tokenizer(cut_text)",
   "id": "b1de3471e62409ec",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:44:33.991667Z",
     "start_time": "2024-12-01T16:44:33.981232Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.vocab_size",
   "id": "f25e20a7a24d4afb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124083"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:44:34.310293Z",
     "start_time": "2024-12-01T16:44:34.294666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"<pad>:\", tokenizer.char2int.get(\"<pad>\"))\n",
    "print(\"<bos>:\", tokenizer.char2int.get(\"<bos>\"))\n",
    "print(\"<eos>:\", tokenizer.char2int.get(\"<eos>\"))\n",
    "# Должно вывести индексы для специальных символов\n"
   ],
   "id": "ecdb8122ad3aa7d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>: 124080\n",
      "<bos>: 124081\n",
      "<eos>: 124082\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T16:44:35.474812Z",
     "start_time": "2024-12-01T16:44:34.833081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "id": "fd2eba06e1e05dd6",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      4\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mJokesDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcut_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[60], line 6\u001B[0m, in \u001B[0;36mJokesDataset.__init__\u001B[1;34m(self, tokenizer, cut_text, max_len)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m tokenizer\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcut_text \u001B[38;5;241m=\u001B[39m cut_text\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<pad>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[66], line 53\u001B[0m, in \u001B[0;36mTokenizer.encode\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     51\u001B[0m lemmatized_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlemmatize(text)\n\u001B[0;32m     52\u001B[0m chars \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<bos>\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(lemmatized_text) \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<eos>\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr_to_idx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchars\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[66], line 43\u001B[0m, in \u001B[0;36mTokenizer.str_to_idx\u001B[1;34m(self, chars)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstr_to_idx\u001B[39m(\u001B[38;5;28mself\u001B[39m, chars):\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Строку в индексы.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchar2int[sym] \u001B[38;5;28;01mfor\u001B[39;00m sym \u001B[38;5;129;01min\u001B[39;00m chars]\n",
      "Cell \u001B[1;32mIn[66], line 43\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstr_to_idx\u001B[39m(\u001B[38;5;28mself\u001B[39m, chars):\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Строку в индексы.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchar2int\u001B[49m\u001B[43m[\u001B[49m\u001B[43msym\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m sym \u001B[38;5;129;01min\u001B[39;00m chars]\n",
      "\u001B[1;31mKeyError\u001B[0m: '<'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:11:22.789949Z",
     "start_time": "2024-12-01T13:11:22.744836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "# Основной цикл обучения\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0  # Суммарные потери за эпоху\n",
    "    model.train()  # Переключение в режим тренировки\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):  # train_loader — DataLoader с батчами\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cuda')\n",
    "        losses.append(loss.item())  # Запись потерь\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Логгирование каждые 100 батчей\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx + 1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Сохранение весов после каждой эпохи\n",
    "    torch.save(model.state_dict(), f\"rnn_epoch_{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Визуализация потерь\n",
    "    plot_losses(losses)\n",
    "\n",
    "# Финальное сохранение модели\n",
    "torch.save(model.state_dict(), \"rnn_final.pt\")\n",
    "print(\"Training completed and model saved.\")"
   ],
   "id": "2daf551788b4e13",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "full() received an invalid combination of arguments - got (tuple, list, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m  \u001B[38;5;66;03m# Суммарные потери за эпоху\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Переключение в режим тренировки\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, train_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):  \u001B[38;5;66;03m# train_loader — DataLoader с батчами\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m training_step(model, train_batch, tokenizer\u001B[38;5;241m.\u001B[39mvocab_size, criterion, optimizer, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())  \u001B[38;5;66;03m# Запись потерь\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\PycharmProjects\\spbu_deep_learning1\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[35], line 15\u001B[0m, in \u001B[0;36mJokesDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     13\u001B[0m encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mencode(text)\n\u001B[0;32m     14\u001B[0m encoded \u001B[38;5;241m=\u001B[39m encoded[:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len]  \u001B[38;5;66;03m# Ограничиваем длину\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m input_sequence \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m target_sequence \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_len,), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_index, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Заполняем входную и целевую последовательность\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: full() received an invalid combination of arguments - got (tuple, list, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, Number fill_value, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:17:59.318322Z",
     "start_time": "2024-11-29T14:17:59.205229Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f\"rnn_bert_epoch_{1}.pt\")",
   "id": "d39d6d1d4172356a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T14:26:47.058224Z",
     "start_time": "2024-11-29T14:26:47.006663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix = \"<bos> \"\n",
    "tokens = torch.tensor(model.tokenizer.encode(prefix), dtype=torch.long, device='cuda').unsqueeze(0)\n",
    "        \n",
    "# Создание one-hot представления\n",
    "inputs = one_hot_encode(tokens, vocab_size=model.vocab_size)\n",
    "\n",
    "# Инициализация скрытого состояния\n",
    "hidden = model.init_hidden(batch_size=1, device='cuda')\n",
    "\n",
    "# Генерация префикса\n",
    "outputs, hidden = model.rnn(inputs, hidden)\n",
    "logits = model.fc(outputs)\n",
    "\n",
    "# Семплирование токена\n",
    "probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "new_token = torch.multinomial(probs, num_samples=1)\n",
    "tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "# Остановка: достижение максимальной длины или EOS-токена\n",
    "while tokens.size(1) < model.max_len and new_token.item() != model.tokenizer.encode('<eos>'):\n",
    "    inputs = one_hot_encode(new_token, vocab_size=model.vocab_size)\n",
    "    outputs, hidden = model.rnn(inputs, hidden)\n",
    "    logits = model.fc(outputs)\n",
    "    probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    new_token = torch.multinomial(probs, num_samples=1)\n",
    "    tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "# Декодирование в строку\n",
    "model.tokenizer.decode(tokens.squeeze().tolist())"
   ],
   "id": "1dd6edac862a05da",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 30522 n 3 k 128 mat1_ld 128 mat2_ld 128 result_ld 30522 abcType 0 computeType 68 scaleType 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Генерация префикса\u001B[39;00m\n\u001B[1;32m     11\u001B[0m outputs, hidden \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mrnn(inputs, hidden)\n\u001B[0;32m---> 12\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Семплирование токена\u001B[39;00m\n\u001B[1;32m     15\u001B[0m probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(logits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/spbu_deep_learning1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/spbu_deep_learning1/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/spbu_deep_learning1/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 30522 n 3 k 128 mat1_ld 128 mat2_ld 128 result_ld 30522 abcType 0 computeType 68 scaleType 0"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6549a7976f47f515"
  },
  {
   "cell_type": "markdown",
   "id": "ef889dd2-fde2-429c-9c61-f5a93517bd3f",
   "metadata": {},
   "source": [
    "Теперь попробуем написать свой собственный RNN. Это будет довольно простая модель с одним слоем.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe4954-e2b5-43c5-9bb6-0b2a5cc2cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: custom model nn.Module, changed CharRNN, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
