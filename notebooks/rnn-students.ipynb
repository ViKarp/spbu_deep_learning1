{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Простейшая рекуррентная сеть\n",
    "В этом ноутбуке мы пройдемся по основам работы с RNN. Сегодня займемся задачей генерации текста. "
   ],
   "id": "39230c0c125c7b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "3d0372f458104003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "В качестве обучающего датасета возьмем набор из 120 тысяч анекдотов на русском языке. \n",
    "[Ссылка на данные](https://archive.org/download/120_tysyach_anekdotov) и [пост на хабре про тематическое моделирование](https://habr.com/ru/companies/otus/articles/723306/)"
   ],
   "id": "2b01304ed592939b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(r\"../additional_materials/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[118:500]"
   ],
   "id": "531fd687e99d2657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Мы не хотим моделировать все подряд, поэтому разобьем датасет на отдельные анекдоты.  ",
   "id": "d8dcf777d273cfb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]"
   ],
   "id": "424cc65ee5b26bf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cut_text = cut_data(text)",
   "id": "eb95ad63654b216f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cut_text[1:6]",
   "id": "e3673ed32e7902a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сделаем для начала самую простую модель с токенами на уровне символов. Это значит, что каждому символу в тексте ставится в соответствие некоторое число. Некоторые способы токенизации используют части слов или, наоборот, части бинарного представления текста.",
   "id": "a830ce525a24ee5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n"
   ],
   "id": "a6f1294f7ea92087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Напишем функции для энкодинга и декодинга нашего текста. Они будут преобразовывать список символов в список чисел и обратно.",
   "id": "85faff74d7ae54ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab[sys] for sys in sentence] # List of ints \n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    return [vocab[toc] for toc in tokens]# list of strings"
   ],
   "id": "83b99e93ea732ddd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "encode(cut_text[0], char2int)",
   "id": "430856c287798779"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Просто представления символов в виде числа не подходят для обучения моделей. На выходе должны быть вероятности всех возможных токенов из словаря. Поэтому модели удобно учить с помощью энтропии. К тому же, токены часто преобразуют из исходного представления в эмбеддинги, которые также позволяют получить более удобное представление в высокоразмерном пространстве. \n",
    "\n",
    "В итоге векторы в модели выглядят следующим образом:\n",
    "![alt_text](../additional_materials/images/char_rnn.jfif)\n",
    "\n",
    "Задание: реализуйте метод, который преобразует батч в бинарное представление."
   ],
   "id": "d2eeb4e360370b9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encodes a batch of sentences (integer indices) into binary one-hot representation.\n",
    "    \n",
    "    Args:\n",
    "        int_words (torch.Tensor): Tensor of size (batch_size, seq_len) containing word indices.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded tensor of size (batch_size, seq_len, vocab_size).\n",
    "    \"\"\"\n",
    "    words_one_hot = torch.zeros(\n",
    "        (int_words.numel(), vocab_size), dtype=torch.float32, device=int_words.device\n",
    "    )\n",
    "    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten().long()] = 1.0\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, vocab_size))\n",
    "    return words_one_hot\n"
   ],
   "id": "2c0cd279cb1b4448"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверьте ваш код.",
   "id": "b6f4f76432e7d0eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_seq = torch.tensor([[2, 6, 4, 1], [0,3, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ],
   "id": "ee4a6dcafff3d37d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Однако, наши последовательности на самом деле разной длины. Как же объединить их в батч?",
   "id": "b3d89f1869ad4071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Реализуем два необходимых класса: \n",
    "- токенайзер, который будет брать текст, кодировать и декодировать символы. Еще одно, что будет реализовано там - добавлено несколько специальных символов (паддинг, конец последовательности, начало последовательности).\n",
    "- Датасет, который будет брать набор шуток, используя токенайзер, строить эмбеддинги и дополнять последовательность до максимальной длины."
   ],
   "id": "67a3f3a5038e7da6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, cut_text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(tuple(set(text))))\n",
    "        self.char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "        self._add_special(\"<pad>\")\n",
    "        self._add_special('<bos>')\n",
    "        self._add_special('<eos>')\n",
    "    \n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.char2int)\n",
    "        self.char2int[symbol] = sym_num\n",
    "        self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2char) # your code\n",
    "        \n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char[el]\n",
    "        \n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int[el]\n",
    "        \n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.char2int[sym] for sym in chars] # str -> list[int]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.int2char[toc] for toc in idx] # list[int] -> list[str]\n",
    "\n",
    "    def encode(self, chars):\n",
    "        chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars) # make string from list"
   ],
   "id": "7bfa0495471297cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode(\"<pad>\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.cut_text[idx]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded = encoded[:self.max_len]  # Ограничиваем длину\n",
    "        input_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        target_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        \n",
    "        # Заполняем входную и целевую последовательность\n",
    "        input_sequence[:len(encoded) - 1] = torch.tensor(encoded[:-1])\n",
    "        target_sequence[1:len(encoded)] = torch.tensor(encoded[1:])\n",
    "        \n",
    "        return input_sequence, target_sequence"
   ],
   "id": "db7a784a2a139f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "id": "616e89b2b5ef727e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Вопрос: А как бы мы должны были разделять данные на последовательности и батчи в случае, если бы использовался сплошной текст?",
   "id": "26851c2eafdff047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for batch in dataloader:\n",
    "    break\n",
    "batch[1].shape"
   ],
   "id": "141707a3641af788"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Теперь реализуем нашу модель. \n",
    "Необходимо следующее:\n",
    " - Используя токенайзер, задать размер словаря\n",
    " - Задать слой RNN с помощью torch.RNN. Доп.задание: создайте модель, используя слой LSTM.\n",
    " - Задать полносвязный слой с набором параметров: размерность ввода — n_hidden; размерность выхода — размер словаря. Этот слой преобразует состояние модели в логиты токенов.\n",
    " - Определить шаг forward, который будет использоваться при обучении\n",
    " - Определить метод init_hidden, который будет задавать начальное внутреннее состояние. Инициализировать будем нулями.\n",
    " - Определить метод inference, в котором будет происходить генерация последовательности из префикса. Здесь мы уже не используем явные логиты, а семплируем токены на их основе.\n"
   ],
   "id": "332b845f0335170d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from typing import Tuple\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Токенизатор для кодирования и декодирования\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size # размер словаря\n",
    "        \n",
    "        # RNN (или LSTM) слой\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.vocab_size,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # Dropout для регуляризации\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # Полносвязный слой: преобразует состояние RNN в логиты\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # One-hot кодирование входной последовательности\n",
    "        x = one_hot_encode(x, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Упаковка последовательностей для эффективности\n",
    "        packed_embeds = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Прогон через LSTM\n",
    "        packed_outputs, hidden = self.rnn(packed_embeds)\n",
    "        \n",
    "        # Распаковка выхода обратно в тензор\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        \n",
    "        # Dropout для регуляризации\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        # Преобразование выхода RNN в логиты\n",
    "        logits = self.fc(outputs)\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int, device: str = \"cpu\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Инициализация начального скрытого состояния нулями\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return h0, c0\n",
    "\n",
    "    def inference(self, prefix=\"<bos> \", device=\"cpu\"):\n",
    "        # Кодирование начального префикса\n",
    "        tokens = torch.tensor(self.tokenizer.encode(prefix), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Создание one-hot представления\n",
    "        inputs = one_hot_encode(tokens, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Инициализация скрытого состояния\n",
    "        hidden = self.init_hidden(batch_size=1, device=device)\n",
    "        \n",
    "        # Генерация префикса\n",
    "        outputs, hidden = self.rnn(inputs, hidden)\n",
    "        logits = self.fc(outputs)\n",
    "        \n",
    "        # Семплирование токена\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "        new_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Остановка: достижение максимальной длины или EOS-токена\n",
    "        while tokens.size(1) < self.max_len and new_token.item() != self.tokenizer.encode('<eos>'):\n",
    "            inputs = one_hot_encode(new_token, vocab_size=self.vocab_size)\n",
    "            outputs, hidden = self.rnn(inputs, hidden)\n",
    "            logits = self.fc(outputs)\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Декодирование в строку\n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())"
   ],
   "id": "6c975b508a3fc327"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Зададим параметры для обучения. Можете варьировать их, чтобы вам хватило ресурсов.",
   "id": "fdaa65b24ad49588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 16\n",
    "seq_length = 512\n",
    "n_hidden = 128\n",
    "n_layers = 6\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ],
   "id": "db640af3cd71f8c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Напишите функцию для одного тренировочного шага. В этом ноутбуке сам процесс обучения модели достаточно тривиален, поэтому мы не будем использовать сложные функции для обучающего цикла. Вы же, однако, можете дописать их.",
   "id": "d1dee9492a0000d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    # Обнуляем градиенты\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Извлекаем данные из пакета\n",
    "    inputs, targets = train_batch\n",
    "    batch_size, seq_len = inputs.shape\n",
    "\n",
    "    # Переносим данные на нужное устройство (например, GPU)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Прямой проход через модель\n",
    "    lengths = (inputs != 0).sum(dim=1)  # или другая логика для определения длин\n",
    "    logits, _ = model(inputs, lengths)  # Получаем логиты от модели\n",
    "\n",
    "    # Переходим от логитов к потере\n",
    "    # targets нужно сдвигать на 1, чтобы правильно сравнить предсказания и настоящие метки\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "    # Обратный проход\n",
    "    loss.backward()\n",
    "\n",
    "    # Обновление весов\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ],
   "id": "2be50c3f8b765c7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Инициализируйте модель, функцию потерь и оптимизатор.",
   "id": "c208fec3adf720b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
   ],
   "id": "da07249e3f178444"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверьте необученную модель: она должна выдавать бессмысленные последовательности",
   "id": "760c69925a454ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()  # Переключаем модель в режим оценки (inference)\n",
    "\n",
    "# Шаг 3: Генерация текста\n",
    "prefix = \"<bos> \"  # Начальный токен последовательности\n",
    "generated_sequence = model.inference(prefix=prefix, device=\"cuda\")\n",
    "\n",
    "# Шаг 4: Вывод результата\n",
    "print(\"Сгенерированная последовательность необученной моделью:\")\n",
    "print(generated_sequence)\n"
   ],
   "id": "e139e152bed0d03e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ],
   "id": "f591c89152ad0b6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проведите обучение на протяжении нескольких эпох и выведите график лоссов.",
   "id": "fc0d709c1e10c9ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for train_batch in dataloader:\n",
    "    inputs, targets = train_batch\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Targets shape: {targets.shape}\")\n",
    "    print(targets)\n",
    "    break"
   ],
   "id": "a40b4ba500e772a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for batch_idx, train_batch in enumerate(dataloader):\n",
    "    inputs, targets = train_batch\n",
    "    print(f\"Batch {batch_idx}: Inputs shape = {inputs.shape}, Targets shape = {targets.shape}\")\n",
    "    break"
   ],
   "id": "45f0b0ae7b6e6e8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "# Основной цикл обучения\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0  # Суммарные потери за эпоху\n",
    "    model.train()  # Переключение в режим тренировки\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):  # train_loader — DataLoader с батчами\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cuda')\n",
    "        losses.append(loss.item())  # Запись потерь\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Логгирование каждые 100 батчей\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx + 1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Сохранение весов после каждой эпохи\n",
    "    torch.save(model.state_dict(), f\"rnn_epoch_{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Визуализация потерь\n",
    "    plot_losses(losses)\n",
    "\n",
    "# Финальное сохранение модели\n",
    "torch.save(model.state_dict(), \"rnn_final.pt\")\n",
    "print(\"Training completed and model saved.\")"
   ],
   "id": "ed6fdee4ef564f0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3210375f71899457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "model.load_state_dict(torch.load('rnn_epoch_1.pt'))"
   ],
   "id": "4c76b630306c1d94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[model.inference(\"<bos>\", device='cuda') for _ in range(10)]",
   "id": "f8f25a1a86dc845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pymorphy2\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, text, max_len: int = 512):\n",
    "        \"\"\"Инициализация токенизатора.\"\"\"\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "\n",
    "        # Уникальные символы\n",
    "        unique_chars = set(text)\n",
    "        self.int2char = dict(enumerate(unique_chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        # Добавление специальных символов\n",
    "        for special in self.specials:\n",
    "            self._add_special(special)\n",
    "\n",
    "        # Инициализация лемматизатора\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    def _add_special(self, symbol: str) -> None:\n",
    "        \"\"\"Добавить специальный символ в словари.\"\"\"\n",
    "        if symbol not in self.char2int:\n",
    "            sym_num = len(self.char2int)\n",
    "            self.char2int[symbol] = sym_num\n",
    "            self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Возвращает размер словаря.\"\"\"\n",
    "        return len(self.int2char)\n",
    "\n",
    "    def decode_symbol(self, el: int) -> str:\n",
    "        \"\"\"Декодирует индекс в символ.\"\"\"\n",
    "        return self.int2char.get(el, '<unk>')  # Возвращает '<unk>', если индекс отсутствует\n",
    "\n",
    "    def encode_symbol(self, el: str) -> int:\n",
    "        \"\"\"Кодирует символ в индекс.\"\"\"\n",
    "        return self.char2int.get(el, self.char2int['<pad>'])  # Если символ отсутствует, подставляет '<pad>'\n",
    "\n",
    "    def str_to_idx(self, chars: str) -> list:\n",
    "        \"\"\"Преобразует строку в список индексов.\"\"\"\n",
    "        return [self.char2int.get(sym, self.char2int['<pad>']) for sym in chars]\n",
    "\n",
    "    def idx_to_str(self, idx: list) -> str:\n",
    "        \"\"\"Преобразует список индексов в строку.\"\"\"\n",
    "        return \"\".join([self.int2char.get(toc, '<unk>') for toc in idx])\n",
    "\n",
    "    def encode(self, text: str) -> list:\n",
    "        \"\"\"Закодировать строку с учетом лемматизации.\"\"\"\n",
    "        lemmatized_text = self.lemmatize(text)\n",
    "        chars = ['<bos>'] + list(lemmatized_text[:self.max_len - 2]) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx: list) -> str:\n",
    "        \"\"\"Декодировать список индексов в строку.\"\"\"\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars).replace('<bos>', '').replace('<eos>', '').strip()\n",
    "\n",
    "    def lemmatize(self, text: str) -> str:\n",
    "        \"\"\"Лемматизировать текст.\"\"\"\n",
    "        words = text.split()\n",
    "        lemmatized = \" \".join([self.morph.parse(word)[0].normal_form for word in words])\n",
    "        return lemmatized\n",
    "\n",
    "    def pad_sequence(self, sequence: list) -> list:\n",
    "        \"\"\"Дополняет или обрезает последовательность до нужной длины.\"\"\"\n",
    "        if len(sequence) < self.max_len:\n",
    "            sequence += [self.char2int['<pad>']] * (self.max_len - len(sequence))\n",
    "        return sequence[:self.max_len]\n"
   ],
   "id": "78f1f2c0d3c4fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenizer = Tokenizer(cut_text)",
   "id": "2d9d185f4aa4893c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a80bac3c9d181da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"<pad>:\", tokenizer.char2int.get(\"<pad>\"))\n",
    "print(\"<bos>:\", tokenizer.char2int.get(\"<bos>\"))\n",
    "print(\"<eos>:\", tokenizer.char2int.get(\"<eos>\"))\n",
    "# Должно вывести индексы для специальных символов\n"
   ],
   "id": "fdcd8834febe8d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob).to('cuda')\n",
    "hidden = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "id": "4f19a973febdf5bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b446bc7365d8a2f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "# Основной цикл обучения\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0  # Суммарные потери за эпоху\n",
    "    model.train()  # Переключение в режим тренировки\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):  # train_loader — DataLoader с батчами\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cuda')\n",
    "        losses.append(loss.item())  # Запись потерь\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Логгирование каждые 100 батчей\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx + 1}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Сохранение весов после каждой эпохи\n",
    "    torch.save(model.state_dict(), f\"rnn_epoch_{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Визуализация потерь\n",
    "    plot_losses(losses)\n",
    "\n",
    "# Финальное сохранение модели\n",
    "torch.save(model.state_dict(), \"rnn_final.pt\")\n",
    "print(\"Training completed and model saved.\")"
   ],
   "id": "e9f201911644228"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), f\"rnn_bert_epoch_{1}.pt\")",
   "id": "a32e199310d54fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prefix = \"<bos> \"\n",
    "tokens = torch.tensor(model.tokenizer.encode(prefix), dtype=torch.long, device='cuda').unsqueeze(0)\n",
    "        \n",
    "# Создание one-hot представления\n",
    "inputs = one_hot_encode(tokens, vocab_size=model.vocab_size)\n",
    "\n",
    "# Инициализация скрытого состояния\n",
    "hidden = model.init_hidden(batch_size=1, device='cuda')\n",
    "\n",
    "# Генерация префикса\n",
    "outputs, hidden = model.rnn(inputs, hidden)\n",
    "logits = model.fc(outputs)\n",
    "\n",
    "# Семплирование токена\n",
    "probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "new_token = torch.multinomial(probs, num_samples=1)\n",
    "tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "# Остановка: достижение максимальной длины или EOS-токена\n",
    "while tokens.size(1) < model.max_len and new_token.item() != model.tokenizer.encode('<eos>'):\n",
    "    inputs = one_hot_encode(new_token, vocab_size=model.vocab_size)\n",
    "    outputs, hidden = model.rnn(inputs, hidden)\n",
    "    logits = model.fc(outputs)\n",
    "    probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    new_token = torch.multinomial(probs, num_samples=1)\n",
    "    tokens = torch.cat([tokens, new_token], dim=1)\n",
    "\n",
    "# Декодирование в строку\n",
    "model.tokenizer.decode(tokens.squeeze().tolist())"
   ],
   "id": "1cee56763a0fa25e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca7f979d654a0ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Теперь попробуем написать свой собственный RNN. Это будет довольно простая модель с одним слоем.\n",
   "id": "e991e6b02a8519ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# YOUR CODE: custom model nn.Module, changed CharRNN, etc",
   "id": "cb3011d0d8792530"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
